{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "##\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords as sw\n",
    "##\n",
    "from sklearn.neighbors import KNeighborsClassifier as knc\n",
    "from sklearn.ensemble import  RandomForestClassifier as rfc\n",
    "from sklearn.naive_bayes import GaussianNB as gnb\n",
    "from sklearn.naive_bayes import ComplementNB as cnb\n",
    "##\n",
    "from sklearn.metrics import confusion_matrix as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the data and using the tokenizer, I decided to save the data in a csv for future use so I do not have to do that waiting again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For importing original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############   TRAINING SET  ###################################\n",
    "#Bring in the X - Values\n",
    "infile = open('data_train.txt', 'r')\n",
    "X = {'X' : [x for x in infile]}\n",
    "infile.close()\n",
    "\n",
    "#Bring in the Y - Values\n",
    "infile = open('labels_train_original.txt', 'r')\n",
    "replace = {'News': 0,\n",
    "           'Opinion': 1,\n",
    "           'Classifieds': 2,\n",
    "           'Features': 3}\n",
    "Y = {'Y' : [replace[y.rstrip('\\n')] for y in infile]}\n",
    "infile.close()\n",
    "\n",
    "#Combine them\n",
    "data = pd.concat([pd.DataFrame(Y), pd.DataFrame(X)], axis = 1)\n",
    "\n",
    "#############   VALIDATE SET  ###################################\n",
    "infile = open('data_valid.txt', 'r')\n",
    "X_test_final = pd.DataFrame({'X' : [x for x in infile]})\n",
    "infile.close()\n",
    "\n",
    "infile = open('labels_valid_original.txt', 'r')\n",
    "Y_test_final = pd.DataFrame({'Y' : [replace[y.rstrip('\\n')] for y in infile]})\n",
    "infile.close()\n",
    "\n",
    "data_test = pd.concat([Y_test_final, X_test_final], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For importing edited data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if importing the already edited data, then there is no reason to go through the transformation pipeline created next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Y                                                  X  length  to the  NNS  \\\n",
      "0  0  the sign in front of the steepled church read ...    4282       0  0.2   \n",
      "1  2  lindsey larsen a soprano and samuel ramey the ...    1933       0  0.0   \n",
      "2  1  to the editor sylvia ann hewlett 's book creat...     745       1  0.0   \n",
      "3  0  illinois tool works inc glenview ill a maker o...    1188       0  0.0   \n",
      "4  1  to the editor robert schaeffer op ed feb 19 ex...     859       1  0.0   \n",
      "\n",
      "   VBP   NN  VBZ   VB  VBD  NNP  \n",
      "0  0.5  0.8  0.5  0.0  0.0  0.0  \n",
      "1  0.0  1.0  1.0  0.0  0.0  0.0  \n",
      "2  0.0  1.0  0.0  1.0  0.0  0.0  \n",
      "3  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "4  1.0  1.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "   Y                                                  X  length  to the  NNS  \\\n",
      "0  1  to the editor re restructuring for security by...     705       1  0.0   \n",
      "1  1  to the editor in small town gay america op ed ...     778       1  0.0   \n",
      "2  1  don king the boxing promoter has stated that m...    4732       0  0.0   \n",
      "3  1  to the editor bill keller god and george w bus...     794       1  0.0   \n",
      "4  0  andres rios stood in front of il monello and r...    2300       0  0.0   \n",
      "\n",
      "   VBP   NN  VBZ   VB  VBD  NNP  \n",
      "0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "1  1.0  1.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "3  1.0  1.0  0.0  0.0  0.0  0.0  \n",
      "4  0.0  1.0  0.0  0.0  0.0  0.0  \n"
     ]
    }
   ],
   "source": [
    "#############   TRAINING SET  ###################################\n",
    "#data = pd.read_csv('data_train_unique_tag.csv')\n",
    "data = pd.read_csv('data_train_unique_s_tag.csv')\n",
    "print(data.head(), '\\n')\n",
    "\n",
    "#############   VALIDATE SET  ###################################\n",
    "#data_test = pd.read_csv('data_valid_unique_tag.csv')\n",
    "data = pd.read_csv('data_valid_unique_s_tag.csv')\n",
    "print(data_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did three custom transformations for the data\n",
    "1) got the length of the article\n",
    "2) 1 or 0 depending on whether the article started with 0\n",
    "3) got the frequency of noun and verb types for the unique words in an article minus the stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class length( BaseEstimator, TransformerMixin ):\n",
    "    \"\"\"\n",
    "    This will return the length of the Article\n",
    "    \"\"\"\n",
    "    #Class Constructor \n",
    "    def __init__( self, get_length = True ):\n",
    "        self._get_length = get_length\n",
    "    \n",
    "    #Return self nothing else to do here    \n",
    "    def fit( self, X, y = None ):\n",
    "        return self \n",
    "    \n",
    "    #Method that describes what we need this transformer to do\n",
    "    def transform( self, X, y = None ):\n",
    "        if self._get_length:\n",
    "            X.loc[:,'length'] = X['X'].apply(lambda x: len(x))\n",
    "        \n",
    "        print('Done Getting length\\n')\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class starts( BaseEstimator, TransformerMixin ):\n",
    "    \"\"\"\n",
    "    This will create a column with a 1 if starts with 'to the'\n",
    "    and 0 if it does not. (opinion articles)\n",
    "    \"\"\"\n",
    "    #Class Constructor \n",
    "    def __init__( self, to_the = True ):\n",
    "        self._to_the = to_the\n",
    "    \n",
    "    #Return self nothing else to do here    \n",
    "    def fit( self, X, y = None ):\n",
    "        return self \n",
    "    \n",
    "    #Method that describes what we need this transformer to do\n",
    "    def transform( self, X, y = None ):\n",
    "        if self._to_the:\n",
    "            X.loc[:,'to the'] = X['X'].apply(lambda x: 1 if x[:6] == 'to the' else 0)\n",
    "\n",
    "        print('Done Getting \"to the\"\\n')\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTK ( BaseEstimator, TransformerMixin ):\n",
    "    \"\"\"\n",
    "    This class takes the articles and looks at the word types of all the unique words in the \n",
    "    article. It then looks specifically at the verbs and nouns and creates a frequency of each \n",
    "    word type for each article\n",
    "    \"\"\"\n",
    "    #Class Constructor \n",
    "    def __init__( self, stopwords = None, keys = None ):\n",
    "        self._stopwords  = stopwords or set(sw.words('english'))\n",
    "        self._keys = keys\n",
    "        \n",
    "    #Return self nothing else to do here    \n",
    "    def fit( self, X, y = None ):\n",
    "        return self \n",
    "    \n",
    "    #Method that converts tagged dictionary to frequency of nouns and verbs\n",
    "    def transform( self, X, y = None ):\n",
    "        \n",
    "        def tokenize_and_tag(X):\n",
    "            #Get the tokens - tokenize, drop stop words, drop repeat words (mostly to reduce time)\n",
    "            tokens = tokenize.word_tokenize(X)\n",
    "            key_words = list(set([words for words in tokens if words not in self._stopwords]))\n",
    "            #key_words = list(set([words for words in key_words if words.isalpha()]))\n",
    "            #Get the tags of each word and count of each of the tags\n",
    "            for words in key_words:\n",
    "                tags = nltk.pos_tag(words)\n",
    "                counts = dict(Counter(tag for word, tag in tags if tag.startswith('N') or tag.startswith('V')))\n",
    "            return counts\n",
    "        #Run the definition created for all values in X\n",
    "        print('Start Tokenizing and Tagging')\n",
    "        counts = X['X'].apply(tokenize_and_tag)\n",
    "        print('Done Tokenizing and Tagging\\n')\n",
    "        \n",
    "        #for training set, self._keys == None, after, the keys will be\n",
    "        #set so that the test set has same column names\n",
    "        print('keys: {}'.format(self._keys))\n",
    "        if self._keys == None:\n",
    "            keys = []\n",
    "            #Go through the articles and collect the uniqe tags in each article\n",
    "            for i in range(len(X)):\n",
    "                interest = list(counts[i].keys())\n",
    "                keys += [x for x in interest if x not in keys]\n",
    "            self._keys = keys\n",
    "            print('Got Keys:\\n{}\\n'.format(self._keys)) \n",
    "        \n",
    "        #Create a dictionary to collect frequencies for easy transfer to Pandas\n",
    "        nvs = {}\n",
    "        for i in self._keys:\n",
    "            nvs[i] = []\n",
    "        #Getting the frequencies of each of the Columns in Data\n",
    "        for i in counts.index:\n",
    "            #sum of verbs (startswith('V')) and nouns (startswith('N'))\n",
    "            vs = sum([counts[i][k] for k in counts[i].keys() if k.startswith('V')])\n",
    "            ns = sum([counts[i][k] for k in counts[i].keys() if k.startswith('N')])\n",
    "            for j in nvs.keys():\n",
    "                #go through column names and if that article has that tag, get frequency\n",
    "                if j in counts[i].keys():\n",
    "                    t = counts[i][j]\n",
    "                    nvs[j] += [t / vs if j.startswith('V') else t / ns]\n",
    "                #if not, set frequency = 0\n",
    "                else:\n",
    "                    nvs[j] += [0]        \n",
    "        print('Got frequencies')\n",
    "        \n",
    "        return pd.concat([X,pd.DataFrame(nvs)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Getting length\n",
      "\n",
      "Done Getting \"to the\"\n",
      "\n",
      "Start Tokenizing and Tagging\n",
      "Done Tokenizing and Tagging\n",
      "\n",
      "keys: None\n",
      "Got Keys:\n",
      "['NN', 'VBZ', 'VBP', 'VB', 'NNS', 'VBD', 'NNP']\n",
      "\n",
      "Got frequencies\n"
     ]
    }
   ],
   "source": [
    "#pipeline - create and transform\n",
    "pipeline = Pipeline(steps = [('length', length()),\n",
    "                             ('starts', starts()),\n",
    "                             ('NLTK', NLTK())])\n",
    "\n",
    "#Try without eliminating stop words\n",
    "pipeline.set_params(NLTK__stopwords = [])\n",
    "\n",
    "data = pipeline.transform(data) # Had to set data = pipeline because\n",
    "                                # not editing dataframe in NLTK transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>length</th>\n",
       "      <th>to the</th>\n",
       "      <th>NN</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>VBP</th>\n",
       "      <th>VB</th>\n",
       "      <th>NNS</th>\n",
       "      <th>VBD</th>\n",
       "      <th>NNP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.497500</td>\n",
       "      <td>2278.365000</td>\n",
       "      <td>0.253500</td>\n",
       "      <td>0.938102</td>\n",
       "      <td>0.083750</td>\n",
       "      <td>0.320542</td>\n",
       "      <td>0.182875</td>\n",
       "      <td>0.026983</td>\n",
       "      <td>0.009833</td>\n",
       "      <td>0.000914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.114726</td>\n",
       "      <td>2013.124167</td>\n",
       "      <td>0.435123</td>\n",
       "      <td>0.192258</td>\n",
       "      <td>0.268343</td>\n",
       "      <td>0.452130</td>\n",
       "      <td>0.375738</td>\n",
       "      <td>0.076939</td>\n",
       "      <td>0.090472</td>\n",
       "      <td>0.011422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>605.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1209.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>4029.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>7231.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Y       length       to the           NN          VBZ  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      1.497500  2278.365000     0.253500     0.938102     0.083750   \n",
       "std       1.114726  2013.124167     0.435123     0.192258     0.268343   \n",
       "min       0.000000    47.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000   605.750000     0.000000     1.000000     0.000000   \n",
       "50%       2.000000  1209.500000     0.000000     1.000000     0.000000   \n",
       "75%       2.000000  4029.250000     1.000000     1.000000     0.000000   \n",
       "max       3.000000  7231.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "               VBP           VB          NNS          VBD          NNP  \n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  \n",
       "mean      0.320542     0.182875     0.026983     0.009833     0.000914  \n",
       "std       0.452130     0.375738     0.076939     0.090472     0.011422  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "75%       1.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "max       1.000000     1.000000     0.500000     1.000000     0.200000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write training set data to a file for future import\n",
    "data.to_csv('data_train_unique_s_tag.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Different Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different base estimators to see which is best. I did this by not even looking at the validate set and split the original Training set to its own Train and test set. I then ran the following Methods for Classifying the data in their base model:\n",
    "1) kNN\n",
    "2) Random Forrest\n",
    "3) Naive Bayes (Gaussian)\n",
    "4) Naive Bayes (Complement)\n",
    "In the original fits I did three steps: Fit, Get Score, Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Train Split and Score Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split test and train of training set\n",
    "X_train, X_test, y_train, y_test = tts(data.loc[:,'length':], data['Y'], stratify = data['Y'])\n",
    "X_train = pd.DataFrame(X_train).reset_index(drop = True)\n",
    "X_test = pd.DataFrame(X_test).reset_index(drop = True)\n",
    "y_train = y_train.reset_index(drop = True)\n",
    "y_test = y_test.reset_index(drop = True)\n",
    "\n",
    "#Score Tracker\n",
    "scores = { 'test' : {},\n",
    "          'train' : {}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kNN (Base) - fit, get scores, store\n",
    "neigh = knc()\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "n_train_score = neigh.score(X_train, y_train)\n",
    "n_test_score = neigh.score(X_test, y_test)\n",
    "\n",
    "scores['test']['knn'] = n_test_score\n",
    "scores['train']['knn'] = n_train_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dustin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#RF (Base) - fit, get scores, store\n",
    "rfc = rfc()\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "rfc_train_score = rfc.score(X_train, y_train)\n",
    "rfc_test_score = rfc.score(X_test, y_test)\n",
    "\n",
    "scores['test']['rfc'] = rfc_test_score\n",
    "scores['train']['rfc'] = rfc_train_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes (Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes (Gaussian) - fit, get scores, store\n",
    "nbg = gnb()\n",
    "nbg.fit(X_train, y_train)\n",
    "\n",
    "nbg_train_score = nbg.score(X_train, y_train)\n",
    "nbg_test_score = nbg.score(X_test, y_test)\n",
    "\n",
    "scores['test']['nbg'] = nbg_test_score\n",
    "scores['train']['nbg'] = nbg_train_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes (Complement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes (Complement) - fit, get scores, store\n",
    "nbc = cnb()\n",
    "nbc.fit(X_train, y_train)\n",
    "\n",
    "nbc_train_score = nbc.score(X_train, y_train)\n",
    "nbc_test_score = nbc.score(X_test, y_test)\n",
    "\n",
    "scores['test']['nbc'] = nbc_test_score\n",
    "scores['train']['nbc'] = nbc_train_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      test     train\n",
      "knn  0.362  0.570000\n",
      "nbc  0.318  0.381333\n",
      "nbg  0.440  0.460667\n",
      "rfc  0.432  0.933333\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I feel there are a couple of options:\n",
    "1) try and play with a kNN or Random Forrest Model\n",
    "2) Just go with the best test Score\n",
    "Time has me starting with 2 and then will do 1 if able."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Training and Validation Data (step 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done by transforming the test data the same training data was transformed. Then fitting the chosen algorithm to the full training set and then making prediction with the test set. Finally look at the confusion matrix for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Getting length\n",
      "\n",
      "Done Getting \"to the\"\n",
      "\n",
      "Start Tokenizing and Tagging\n",
      "Done Tokenizing and Tagging\n",
      "\n",
      "keys: ['NN', 'VBZ', 'VBP', 'VB', 'NNS', 'VBD', 'NNP']\n",
      "Got frequencies\n"
     ]
    }
   ],
   "source": [
    "#Transform Validate data\n",
    "data_test = pipeline.transform(data_test)\n",
    "\n",
    "#Split into X and Y\n",
    "X_test_final = data_test.loc[:, 'length':]\n",
    "Y_test_final = data_test['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>length</th>\n",
       "      <th>to the</th>\n",
       "      <th>NN</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>VBP</th>\n",
       "      <th>VB</th>\n",
       "      <th>NNS</th>\n",
       "      <th>VBD</th>\n",
       "      <th>NNP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.490000</td>\n",
       "      <td>2232.154500</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>0.940164</td>\n",
       "      <td>0.084750</td>\n",
       "      <td>0.326542</td>\n",
       "      <td>0.190792</td>\n",
       "      <td>0.026020</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.001316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.128511</td>\n",
       "      <td>1979.238558</td>\n",
       "      <td>0.435406</td>\n",
       "      <td>0.188066</td>\n",
       "      <td>0.268288</td>\n",
       "      <td>0.455184</td>\n",
       "      <td>0.383879</td>\n",
       "      <td>0.074551</td>\n",
       "      <td>0.089402</td>\n",
       "      <td>0.014053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>611.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1132.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>4025.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>7132.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Y       length       to the           NN          VBZ  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      1.490000  2232.154500     0.254000     0.940164     0.084750   \n",
       "std       1.128511  1979.238558     0.435406     0.188066     0.268288   \n",
       "min       0.000000    56.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000   611.000000     0.000000     1.000000     0.000000   \n",
       "50%       1.000000  1132.000000     0.000000     1.000000     0.000000   \n",
       "75%       3.000000  4025.500000     1.000000     1.000000     0.000000   \n",
       "max       3.000000  7132.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "               VBP           VB          NNS          VBD          NNP  \n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  \n",
       "mean      0.326542     0.190792     0.026020     0.010417     0.001316  \n",
       "std       0.455184     0.383879     0.074551     0.089402     0.014053  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "75%       1.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "max       1.000000     1.000000     0.500000     1.000000     0.200000  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.to_csv('data_valid_unique_s_tag.csv', index = False)\n",
    "data_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training set\n",
    "X_train_final = data.loc[:,'length':]\n",
    "y_train_final = data['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the desired Algorithm\n",
    "nbg = gnb()\n",
    "nbg.fit(X_train_final, y_train_final)\n",
    "\n",
    "#Predict\n",
    "Y_test_predict = nbg.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  34 476   0]\n",
      " [  7 391 109   0]\n",
      " [  1   7 461   1]\n",
      " [  2  83 426   0]]\n",
      "\n",
      "[[0.00390625 0.06640625 0.9296875  0.        ]\n",
      " [0.01380671 0.77120316 0.21499014 0.        ]\n",
      " [0.00212766 0.01489362 0.98085106 0.00212766]\n",
      " [0.00391389 0.16242661 0.83365949 0.        ]]\n",
      "\n",
      "Global Accuracy of 0.44\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix - Standard Confusion, Percent Confusion\n",
    "confusion = cm(Y_test_final, Y_test_predict)\n",
    "fun = lambda x: x/sum(x)\n",
    "cm_perc = np.apply_along_axis(fun, 1, confusion)\n",
    "\n",
    "#Get the global Accuracy\n",
    "global_acc = sum(cm_perc.diagonal())/4\n",
    "\n",
    "print('{}\\n\\n{}\\n\\nGlobal Accuracy of {:.2f}'.format(confusion, cm_perc, global_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It did not classify anything as an article or feature. Not very helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest parameters (step 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep track of the scores\n",
    "rf_scores1 = {\n",
    "    'test' : {},\n",
    "    'train' : {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dustin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Dustin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Dustin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Dustin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Play with min samples split\n",
    "min_samples_split = [11, 12, 13, 14]\n",
    "\n",
    "for i in min_samples_split:\n",
    "    from sklearn.ensemble import  RandomForestClassifier as rfc\n",
    "    #fit\n",
    "    rfc = rfc(min_samples_split = i).fit(X_train, y_train)\n",
    "    #Scores\n",
    "    rfc_train_score = rfc.score(X_train, y_train)\n",
    "    rfc_test_score = rfc.score(X_test, y_test)\n",
    "    #Store\n",
    "    rf_scores1['test']['{}'.format(i)] = rfc_test_score\n",
    "    rf_scores1['train']['{}'.format(i)] = rfc_train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     test     train\n",
      "11  0.472  0.706000\n",
      "12  0.468  0.682667\n",
      "13  0.492  0.682667\n",
      "14  0.460  0.676000\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(rf_scores1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_scores2 = {\n",
    "    'test' : {},\n",
    "    'train' : {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dustin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Dustin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Dustin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Dustin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Dustin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Play with min samples leaf\n",
    "min_samples_leaf = [15, 17, 19, 21, 23]\n",
    "\n",
    "for i in min_samples_leaf:\n",
    "    from sklearn.ensemble import  RandomForestClassifier as rfc\n",
    "    #fit\n",
    "    rfc = rfc(min_samples_split = 13, min_samples_leaf = i).fit(X_train, y_train)\n",
    "    #Scores\n",
    "    rfc_train_score = rfc.score(X_train, y_train)\n",
    "    rfc_test_score = rfc.score(X_test, y_test)\n",
    "    #Store\n",
    "    rf_scores2['test']['{}'.format(i)] = rfc_test_score\n",
    "    rf_scores2['train']['{}'.format(i)] = rfc_train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     test     train\n",
      "15  0.470  0.550000\n",
      "17  0.480  0.532667\n",
      "19  0.474  0.545333\n",
      "21  0.492  0.534667\n",
      "23  0.482  0.528667\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(rf_scores2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following values for parameters, I will run a final estimator and look at the results from a confusion matrix.\n",
    "1) min_leaf_split = 13\n",
    "1) min_leaf_samples = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dustin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import  RandomForestClassifier as rfc\n",
    "#fit\n",
    "rfc = rfc(min_samples_split = 13, min_samples_leaf = 21).fit(X_train_final, y_train_final)\n",
    "#Predict\n",
    "Y_test_predict_rfc = rfc.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[177  28 215  92]\n",
      " [ 51 397  12  47]\n",
      " [107   3 282  78]\n",
      " [125  80 198 108]]\n",
      "\n",
      "[[0.34570312 0.0546875  0.41992188 0.1796875 ]\n",
      " [0.10059172 0.78303748 0.02366864 0.09270217]\n",
      " [0.22765957 0.00638298 0.6        0.16595745]\n",
      " [0.2446184  0.15655577 0.38747554 0.21135029]]\n",
      "\n",
      "Global Accuracy of 0.49\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix - Standard Confusion, Percent Confusion\n",
    "confusion2 = cm(Y_test_final, Y_test_predict_rfc)\n",
    "fun = lambda x: x/sum(x)\n",
    "cm_perc2 = np.apply_along_axis(fun, 1, confusion2)\n",
    "\n",
    "#Get the global Accuracy\n",
    "global_acc2 = sum(cm_perc2.diagonal())/4\n",
    "\n",
    "print('{}\\n\\n{}\\n\\nGlobal Accuracy of {:.2f}'.format(confusion2, cm_perc2, global_acc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Train Split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
